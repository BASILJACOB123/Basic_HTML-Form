{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIm3thXDz8Lr3RLDX9iquP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BASILJACOB123/Basic_HTML-Form/blob/master/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CbJVsoS4B1v",
        "outputId": "0d3931d1-b3d0-4609-9475-2b727de9e528"
      },
      "source": [
        "import numpy as np\n",
        "a=np.random.randn(5,1)\n",
        "a"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.33816397],\n",
              "       [ 0.75549839],\n",
              "       [ 0.68795103],\n",
              "       [-0.18907016],\n",
              "       [ 0.68461601]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9rozg2h4Ea6",
        "outputId": "c9b62519-aa7e-46cf-8c74-51f0de8a3f08"
      },
      "source": [
        "print(a.T)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.33816397  0.75549839  0.68795103 -0.18907016  0.68461601]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReOtkIZa4RQQ",
        "outputId": "37f64874-8c8f-492c-e03c-d637e26be9ab"
      },
      "source": [
        "print(np.dot(a,a.T))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.79068281 -1.01098072 -0.92059128  0.25300688 -0.91612848]\n",
            " [-1.01098072  0.57077782  0.51974589 -0.1428422   0.5172263 ]\n",
            " [-0.92059128  0.51974589  0.47327662 -0.13007101  0.47098229]\n",
            " [ 0.25300688 -0.1428422  -0.13007101  0.03574753 -0.12944046]\n",
            " [-0.91612848  0.5172263   0.47098229 -0.12944046  0.46869909]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrK8XtAe4T1S"
      },
      "source": [
        "assert(a.shape==(5,1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r6jnIcT4i0F"
      },
      "source": [
        "import math\n",
        "\n",
        "def basic_sigmoid(x):\n",
        "\n",
        "  sig=1/(1+math.exp(-x))\n",
        "\n",
        "  return sig "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdL3RzFt7Wlc",
        "outputId": "cd17d2d3-3b17-46e5-d4b9-1101d5c248ec"
      },
      "source": [
        "basic_sigmoid(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9525741268224334"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS7REky47iv6",
        "outputId": "2da338a6-644f-4328-c3db-8a00ce2f43a6"
      },
      "source": [
        "x=np.array([1,2,3])\n",
        "1/1+np.exp(-x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.36787944, 1.13533528, 1.04978707])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVh5DAnM7mdV",
        "outputId": "d74f3d74-7afa-4804-cd65-8b6c6bdde316"
      },
      "source": [
        "print(x+3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2kCiLLB8Cvk"
      },
      "source": [
        "def sigmoid(x):\n",
        "\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmTlsWbp8K28",
        "outputId": "944809e6-6cda-4533-bcba-d883fa5d18b4"
      },
      "source": [
        "x=np.array([1,2,3])\n",
        "sigmoid(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAt-7Wxj8PB5"
      },
      "source": [
        "def sigmoid_derivative(x):\n",
        "\n",
        "  a=sigmoid(x)\n",
        "  da=a*(1-a)\n",
        "\n",
        "  return da"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfIBu5Yj8q7O",
        "outputId": "bc141ac4-b695-4b55-b282-195aee628d3c"
      },
      "source": [
        "x=np.array([1,2,3])\n",
        "sigmoid_derivative(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.19661193, 0.10499359, 0.04517666])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH3s95L99LMp"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpXuAsn59GPF"
      },
      "source": [
        " Reshaping arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_NXvtZH9IaI"
      },
      "source": [
        "def image2vector(image):\n",
        "  \"\"\"\n",
        "    Argument:\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "    \n",
        "    Returns:\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "  \"\"\"\n",
        "  \n",
        "  v=image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
        "\n",
        "  return v"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmynftJD9aSY",
        "outputId": "72022b7b-d191-4829-f6b9-37f4335583cf"
      },
      "source": [
        "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (\"image2vector(image) = \" + str(image2vector(image)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image2vector(image) = [[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1orr55P-N9c"
      },
      "source": [
        "Normalizing rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbIv0G-uJRoA"
      },
      "source": [
        "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
        "e, if$$x = \n",
        "\\begin{bmatrix}\n",
        "    0 &amp; 3 &amp; 4 \\\\\n",
        "    2 &amp; 6 &amp; 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$then$$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$and$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 &amp; \\frac{3}{5} &amp; \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} &amp; \\frac{6}{\\sqrt{56}} &amp; \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$\n",
        "\n",
        "\n",
        "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n",
        "\n",
        "Exercise: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJAU9yXgJOD9"
      },
      "source": [
        "def normalizeRows(x):\n",
        "  \"\"\"\n",
        "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
        "    \n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "    \n",
        "    Returns:\n",
        "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
        "  \"\"\"\n",
        "  x_norm=np.linalg.norm(x,axis=1,keepdims=True)\n",
        "\n",
        "  x=x/x_norm\n",
        "    \n",
        "  print(x.shape,x_norm.shape)\n",
        "\n",
        "\n",
        "  return x\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjoCxKpNJ9nS",
        "outputId": "4bedc665-0ba1-498b-a239-6407d5e2c072"
      },
      "source": [
        "x=np.array([\n",
        "            [0,3,4],\n",
        "            [1,6,4]\n",
        "])\n",
        "\n",
        "print(\"normalizeRows(x) = \"+str(normalizeRows(x)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3) (2, 1)\n",
            "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a-KDzkrKffE"
      },
      "source": [
        "**Broadcasting and the softmax function**\n",
        "\n",
        "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official broadcasting documentation.\n",
        "\n",
        "Exercise: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "$ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
        "  x_1  &amp;&amp;\n",
        "  x_2 &amp;&amp;\n",
        "  ...  &amp;&amp;\n",
        "  x_n  \n",
        "\\end{bmatrix}) = \\begin{bmatrix}\n",
        "   \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &amp;&amp;\n",
        "  \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &amp;&amp;\n",
        "  ...  &amp;&amp;\n",
        "  \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
        "\\end{bmatrix} $\n",
        "\n",
        "$\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{, $x{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$ $$softmax(x) = softmax\\begin{bmatrix} x{11} & x{12} & x{13} & \\dots & x{1n} \\ x{21} & x{22} & x{23} & \\dots & x{2n} \\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\ x{m1} & x{m2} & x{m3} & \\dots & x{mn} \\end{bmatrix} = \\begin{bmatrix} \\frac{e^{x{11}}}{\\sum{j}e^{x{1j}}} & \\frac{e^{x{12}}}{\\sum{j}e^{x{1j}}} & \\frac{e^{x{13}}}{\\sum{j}e^{x{1j}}} & \\dots & \\frac{e^{x{1n}}}{\\sum{j}e^{x{1j}}} \\ \\frac{e^{x{21}}}{\\sum{j}e^{x{2j}}} & \\frac{e^{x{22}}}{\\sum{j}e^{x{2j}}} & \\frac{e^{x{23}}}{\\sum{j}e^{x{2j}}} & \\dots & \\frac{e^{x{2n}}}{\\sum{j}e^{x{2j}}} \\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\ \\frac{e^{x{m1}}}{\\sum{j}e^{x{mj}}} & \\frac{e^{x{m2}}}{\\sum{j}e^{x{mj}}} & \\frac{e^{x{m3}}}{\\sum{j}e^{x{mj}}} & \\dots & \\frac{e^{x{mn}}}{\\sum{j}e^{x_{mj}}} \\end{bmatrix} =$$\\begin{pmatrix}\n",
        "  softmax\\text{(first row of x)}  \\\\\n",
        "  softmax\\text{(second row of x)} \\\\\n",
        "  ...  \\\\\n",
        "  softmax\\text{(last row of x)} \\\\\n",
        "\\end{pmatrix}$$$$\n",
        "\n",
        "Note\n",
        "Note that later in the course, you'll see \"m\" used to represent the \"number of training examples\", and each training example is in its own column of the matrix.\n",
        "Also, each feature will be in its own row (each row has data for the same feature).\n",
        "Softmax should be performed for all features of each training example, so softmax would be performed on the columns (once we switch to that representation later in this course).\n",
        "\n",
        "However, in this coding practice, we're just focusing on getting familiar with Python, so we're using the common math notation $m \\times n$\n",
        "where $m$ is the number of rows and $n$ is the number of columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u9TPiQAKgAZ"
      },
      "source": [
        "def softmax(x):\n",
        "\n",
        "\n",
        "  ### START CODE HERE ### (≈ 3 lines of code)\n",
        "  # Apply exp() element-wise to x. Use np.exp(...).\n",
        "\n",
        "  x_exp=np.exp(x)\n",
        "\n",
        "  # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
        "  x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
        "  \n",
        "  # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
        "\n",
        "  s=x_exp/x_sum\n",
        "\n",
        "  return s\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GG-nZnMMKzg",
        "outputId": "133cad88-1747-4dd1-967d-e2595892735e"
      },
      "source": [
        "x = np.array([\n",
        "    [9, 2, 5, 0, 0],\n",
        "    [7, 5, 0, 0 ,0]])\n",
        "print(\"softmax(x) = \" + str(softmax(x)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
            "  1.21052389e-04]\n",
            " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
            "  8.01252314e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_rbD3deMMRP"
      },
      "source": [
        "A=np.array([[1,2,3],\n",
        "           [4,5,6],\n",
        "            [7,8,9]])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyz095-_M6iB",
        "outputId": "0d9790fb-40a7-4522-e29d-437c48926603"
      },
      "source": [
        "A.sum(axis=0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 15, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMX6Tb8OM7s9",
        "outputId": "3c85372e-3d44-46ca-df47-046d656653c8"
      },
      "source": [
        "A.sum(axis=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 15, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vA_QUSYM-3a",
        "outputId": "8a4dc91f-7934-4db3-e701-9da358278160"
      },
      "source": [
        "x_exp=np.exp(x)\n",
        "x_exp"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.10308393e+03, 7.38905610e+00, 1.48413159e+02, 1.00000000e+00,\n",
              "        1.00000000e+00],\n",
              "       [1.09663316e+03, 1.48413159e+02, 1.00000000e+00, 1.00000000e+00,\n",
              "        1.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oru46EnKNor6",
        "outputId": "8686cb14-121a-45e0-fc46-45a2c7f3a591"
      },
      "source": [
        "np.sum(x_exp, axis=1, keepdims=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8260.88614278],\n",
              "       [1248.04631753]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXVnZ8B0OgVo"
      },
      "source": [
        "**Vectorization**\n",
        "\n",
        "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWJa1hgVOjOq",
        "outputId": "f9ad11ce-1627-485a-e062-fb30ac66182e"
      },
      "source": [
        "import time\n",
        "\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(x1)):\n",
        "    dot+= x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        outer[i,j] = x1[i]*x2[j]\n",
        "toc = time.process_time()\n",
        "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.zeros(len(x1))\n",
        "for i in range(len(x1)):\n",
        "    mul[i] = x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
        "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
        "tic = time.process_time()\n",
        "gdot = np.zeros(W.shape[0])\n",
        "for i in range(W.shape[0]):\n",
        "    for j in range(len(x1)):\n",
        "        gdot[i] += W[i,j]*x1[j]\n",
        "toc = time.process_time()\n",
        "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dot = 278\n",
            " ----- Computation time = 0.07190999999906467ms\n",
            "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            " ----- Computation time = 0.19532099999963748ms\n",
            "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            " ----- Computation time = 0.13576299999940034ms\n",
            "gdot = [27.22037632 34.89135059 18.08314935]\n",
            " ----- Computation time = 0.1577909999994631ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtwlloqXPCmO"
      },
      "source": [
        "Implement the L1 and L2 loss functions\n",
        "\n",
        "\n",
        "Exercise: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
        "\n",
        "Reminder:\n",
        "\n",
        "The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
        "L1 loss is defined as:$$\\begin{align*} &amp; L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ABgKcs5Orvj"
      },
      "source": [
        "def L1(yhat,y):\n",
        "\n",
        "  loss=np.sum(abs(yhat-y))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkM8xse-PTvF",
        "outputId": "30d6aef9-668c-4cbc-9866-dca1213e9588"
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L1 = \" + str(L1(yhat,y)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 = 1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjFe9_dJPaVG"
      },
      "source": [
        "\n",
        "Exercise: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then np.dot(x,x) = $\\sum_{j=0}^n x_j^{2}$.\n",
        "\n",
        "L2 loss is defined as$$\\begin{align*} &amp; L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E52jBz8FPWtX"
      },
      "source": [
        "def L2(yhat,y):\n",
        "\n",
        "  loss=np.sum(np.dot(yhat-y,yhat-y))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pdbh2L5PoPF",
        "outputId": "8b97b535-9987-485b-f436-f85a69cd2854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L2 = \" + str(L2(yhat,y)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 = 0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKopG5E1PufN"
      },
      "source": [
        "\n",
        "Congratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!\n",
        "\n",
        "Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
        "You have reviewed the L1 and L2 loss.\n",
        "You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2peWMg8PwCW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}